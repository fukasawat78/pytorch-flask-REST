{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Deep Neural Network with Optuna's Parameter Tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[[1.8722, 1.8722, 1.8722,  ..., 1.9235, 1.9235, 1.9407],\n          [1.8722, 1.8722, 1.8722,  ..., 1.9235, 1.9235, 1.9407],\n          [1.8722, 1.8722, 1.8722,  ..., 1.9235, 1.9235, 1.9407],\n          ...,\n          [0.5022, 0.5193, 0.6049,  ..., 0.5707, 0.6221, 0.6221],\n          [0.4337, 0.5536, 0.6563,  ..., 0.6049, 0.6392, 0.6734],\n          [0.4508, 0.5536, 0.6221,  ..., 0.6392, 0.6049, 0.6392]],\n\n         [[2.0259, 2.0259, 2.0259,  ..., 2.0784, 2.0784, 2.0434],\n          [2.0259, 2.0259, 2.0259,  ..., 2.0784, 2.0784, 2.0434],\n          [2.0259, 2.0259, 2.0259,  ..., 2.0784, 2.0784, 2.0434],\n          ...,\n          [0.4503, 0.4853, 0.5728,  ..., 0.3803, 0.4328, 0.4328],\n          [0.3803, 0.5203, 0.6254,  ..., 0.4153, 0.4503, 0.4853],\n          [0.3978, 0.5203, 0.5903,  ..., 0.4503, 0.4153, 0.4503]],\n\n         [[2.1694, 2.1694, 2.1694,  ..., 2.2217, 2.2217, 2.2217],\n          [2.1694, 2.1694, 2.1694,  ..., 2.2217, 2.2217, 2.2217],\n          [2.1694, 2.1694, 2.1694,  ..., 2.2217, 2.2217, 2.2217],\n          ...,\n          [0.4788, 0.5485, 0.6356,  ..., 0.3045, 0.3219, 0.3568],\n          [0.4265, 0.5659, 0.6705,  ..., 0.3393, 0.3393, 0.4091],\n          [0.4265, 0.5311, 0.6182,  ..., 0.3742, 0.3045, 0.3568]]]])\n"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def transform_image(image_bytes):\n",
    "    my_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            [0.485, 0.456, 0.406],\n",
    "                                            [0.229, 0.224, 0.225])])\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    return my_transforms(image).unsqueeze(0)\n",
    "\n",
    "with open(\"./static/cat.jpg\", 'rb') as f:\n",
    "    image_bytes = f.read()\n",
    "    tensor = transform_image(image_bytes=image_bytes)\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /Users/fukasawat78/.cache/torch/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:32<00:00, 996kB/s]\n"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Make sure to pass `pretrained` as `True` to use the pretrained weights:\n",
    "model = models.densenet121(pretrained=True)\n",
    "# Since we are using our model only for inference, switch to `eval` mode:\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def get_prediction(image_bytes):\n",
    "    tensor = transform_image(image_bytes=image_bytes)\n",
    "    outputs = model.forward(tensor)\n",
    "    _, y_hat = outputs.max(1)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "imagenet_class_index = json.load(open('./static/imagenet_class_index.json'))\n",
    "\n",
    "def get_prediction(image_bytes):\n",
    "    tensor = transform_image(image_bytes=image_bytes)\n",
    "    outputs = model.forward(tensor)\n",
    "    _, y_hat = outputs.max(1)\n",
    "    predicted_idx = str(y_hat.item())\n",
    "    return imagenet_class_index[predicted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['n02124075', 'Egyptian_cat']\n"
    }
   ],
   "source": [
    "with open(\"./static/cat.jpg\", 'rb') as f:\n",
    "    image_bytes = f.read()\n",
    "    print(get_prediction(image_bytes=image_bytes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}